# litellm-config/config.yaml

model_list:
  # OpenAI models (already have key)
  - model_name: gpt-4-turbo
    litellm_params:
      model: gpt-4-turbo-preview
      api_key: ${OPENAI_API_KEY}

  - model_name: gpt-3.5-turbo
    litellm_params:
      model: gpt-3.5-turbo
      api_key: ${OPENAI_API_KEY}

  # Claude (when you add Anthropic key)
  - model_name: claude-sonnet
    litellm_params:
      model: claude-sonnet-4-20250514
      api_key: ${ANTHROPIC_API_KEY}

  # Azure OpenAI (if you switch to Azure)
  # - model_name: azure-gpt4
  #   litellm_params:
  #     model: azure/gpt-4-turbo
  #     api_base: ${AZURE_API_BASE}
  #     api_key: ${AZURE_API_KEY}
  #     api_version: "2024-02-15-preview"

# General settings
litellm_settings:
  drop_params: true  # Drop unsupported params instead of erroring
  set_verbose: false
  success_callback: ["langfuse"]  # Optional: for analytics

# Router settings (load balancing, fallbacks)
router_settings:
  routing_strategy: least-busy  # or: simple-shuffle, latency-based
  num_retries: 3
  timeout: 60
  allowed_fails: 3

# Rate limiting per user/key
general_settings:
  master_key: ${LITELLM_MASTER_KEY}  # Protect the proxy

# Per-user rate limits (queries per minute)
litellm_settings:
  max_parallel_requests: 100
  global_max_parallel_requests: 1000
